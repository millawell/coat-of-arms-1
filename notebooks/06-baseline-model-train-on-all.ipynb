{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e5cdbcf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# auto-load when code changes outside\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%load_ext pyinstrument\n",
    "\n",
    "import sys\n",
    "sys.path.insert(0,'..')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "12fa6a61",
   "metadata": {},
   "outputs": [],
   "source": [
    "#imports \n",
    "import os\n",
    "import nltk\n",
    "import spacy\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.onnx as onnx\n",
    "import torch.optim as optim\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as T\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "\n",
    "from torch.utils.data import DataLoader,Dataset\n",
    "from PIL import Image\n",
    "from datetime import datetime\n",
    "from tqdm import tqdm\n",
    "from time import sleep\n",
    "\n",
    "from src.baseline.vocabulary import Vocabulary\n",
    "from src.baseline.data_loader import get_loader, get_loaders, get_mean_std\n",
    "from src.baseline.coa_model import save_model, get_new_model, validate_model, train_validate_test_split, print_time\n",
    "\n",
    "from pyinstrument import Profiler\n",
    "\n",
    "# torch.cuda.empty_cache()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "814ad5d2",
   "metadata": {},
   "source": [
    "## Torch data-loader\n",
    "https://www.kaggle.com/mdteach/torch-data-loader-flicker-8k"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "587ed02f",
   "metadata": {},
   "source": [
    "# Split Data into Training/Test datasets "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c61fc8d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 137 total images\n",
      "There are 82 train images\n",
      "There are 27 val images\n",
      "There are 28 test images\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# data_location = '/home/space/datasets/COA/generated-data-api'\n",
    "data_location =  '/home/space/datasets/COA/generated-data-api-small'\n",
    "# data_location =  '../generated-data-api-small'\n",
    "caption_file = data_location + '/captions.txt'\n",
    "root_folder_images = data_location + '/images'\n",
    "df = pd.read_csv(caption_file)\n",
    "\n",
    "train, validate, test = train_validate_test_split(df, train_percent=.6, validate_percent=.2, seed=None)\n",
    "\n",
    "\n",
    "train_annotation_file = data_location + '/train_captions.txt'\n",
    "val_annotation_file  = data_location + '/val_captions.txt'\n",
    "test_annotation_file  = data_location + '/test_captions.txt'\n",
    "\n",
    "train.to_csv(train_annotation_file, sep=',',index=False)\n",
    "test.to_csv(test_annotation_file, sep=',',index=False)\n",
    "validate.to_csv(val_annotation_file, sep=',',index=False)\n",
    "\n",
    "\n",
    "print(\"There are {} total images\".format(len(df)))\n",
    "\n",
    "caption_file = data_location + '/train_captions.txt'\n",
    "df1 = pd.read_csv(caption_file)\n",
    "print(\"There are {} train images\".format(len(df1)))\n",
    "\n",
    "caption_file = data_location + '/val_captions.txt'\n",
    "df2 = pd.read_csv(caption_file)\n",
    "print(\"There are {} val images\".format(len(df2)))\n",
    "\n",
    "caption_file = data_location + '/test_captions.txt'\n",
    "df3 = pd.read_csv(caption_file)\n",
    "print(\"There are {} test images\".format(len(df3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e779ff1a",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Caption: image_2.png A A lion rampant\n",
      "(500, 500, 4)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "for i in range(1, 2):\n",
    "    image_path = data_location + '/images/'+ df.iloc[i,0]\n",
    "    print(\"Caption:\", df.iloc[i,0], df.iloc[i,1])\n",
    "    img=mpimg.imread(image_path)\n",
    "    print(img.shape)\n",
    "    plt.imshow(img)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "25fd47a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "!export CUDA_VISIBLE_DEVICES=0\n",
    "!export CUDA_LAUNCH_BLOCKING=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cf67d490",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "# device = torch.device(\"cpu\")\n",
    "\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0a13d222",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Tesla P100-PCIE-12GB'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.get_device_name(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f8bbb754",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "\n",
    "gc.collect()\n",
    "\n",
    "torch.cuda.empty_cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d3b51731",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.multiprocessing as mp\n",
    "\n",
    "mp.set_start_method('spawn')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78024cbd",
   "metadata": {},
   "source": [
    "## Zipping images to (/tmp) folder on the node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0a5078f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " ------------------------ \n",
      " started Zipping images @ Time = 13:25:24\n",
      "\n",
      " ------------------------ \n",
      " finished Zipping images @ Time = 13:25:25\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import zipfile\n",
    "from shutil import copyfile\n",
    "\n",
    "def zipdir(path, ziph):\n",
    "    # ziph is zipfile handle\n",
    "    for root, dirs, files in os.walk(path):\n",
    "        for file in files:\n",
    "            ziph.write(os.path.join(root, file), \n",
    "                       os.path.relpath(os.path.join(root, file), \n",
    "                                       os.path.join(path, '..')))\n",
    "            \n",
    "\n",
    "print_time('\\n ------------------------ \\n started Zipping images')\n",
    "\n",
    "path_to_zip_file = '/tmp/coa-images-alln.zip'\n",
    "zipf = zipfile.ZipFile(path_to_zip_file, 'w', zipfile.ZIP_DEFLATED)\n",
    "zipdir(root_folder_images, zipf)\n",
    "zipf.close()\n",
    "print_time('\\n ------------------------ \\n finished Zipping images')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0be741a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "tmp_data_location = '/tmp/generated-data-api'\n",
    "# tmp_data_location =  '/tmp/generated-data-api-small'\n",
    "tmp_root_folder_images = tmp_data_location + '/images'\n",
    "\n",
    "tmp_train_annotation_file = tmp_data_location + '/train_captions.txt'\n",
    "tmp_val_annotation_file  = tmp_data_location + '/val_captions.txt'\n",
    "tmp_test_annotation_file  = tmp_data_location + '/test_captions.txt'\n",
    "tmp_caption_file = tmp_data_location + '/captions.txt'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4adf4c8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "print_time('\\n ------------------------ \\n started unzipping dataset')\n",
    "\n",
    "with zipfile.ZipFile(path_to_zip_file, 'r') as zip_ref:\n",
    "    zip_ref.extractall(tmp_data_location)\n",
    "\n",
    "print_time('\\n ------------------------ \\n finished unzipping dataset')\n",
    "\n",
    "\n",
    "copyfile(caption_file, tmp_caption_file)\n",
    "copyfile(train_annotation_file, tmp_train_annotation_file)\n",
    "copyfile(val_annotation_file, tmp_val_annotation_file)\n",
    "copyfile(test_annotation_file, tmp_test_annotation_file)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d1e7cfd",
   "metadata": {},
   "source": [
    "# Image Captioning With Attention - Pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e5cc0bba",
   "metadata": {},
   "outputs": [],
   "source": [
    "#setting the constants\n",
    "BATCH_SIZE = 128\n",
    "# BATCH_SIZE = 5\n",
    "NUM_WORKER = 2 #### this needs multi-core\n",
    "freq_threshold = 5\n",
    "# 30 minutes to create those, as it's baseline, i ran it several times and it's the same\n",
    "vocab = Vocabulary(freq_threshold)\n",
    "vocab.stoi = {'<PAD>': 0, '<SOS>': 1, '<EOS>': 2, '<UNK>': 3, 'g': 4, 'v': 5, 'b': 6, 'cross': 7, 'lion': 8, 'passt': 9, 's': 10, 'a': 11, 'eagle': 12, 'o': 13, 'doubleheaded': 14, \"'s\": 15, 'head': 16, 'patonce': 17, 'moline': 18, 'guard': 19, 'rampant': 20}\n",
    "vocab.itos = {0: '<PAD>', 1: '<SOS>', 2: '<EOS>', 3: '<UNK>', 4: 'g', 5: 'v', 6: 'b', 7: 'cross', 8: 'lion', 9: 'passt', 10: 's', 11: 'a', 12: 'eagle', 13: 'o', 14: 'doubleheaded', 15: \"'s\", 16: 'head', 17: 'patonce', 18: 'moline', 19: 'guard', 20: 'rampant'}\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24d3afbe",
   "metadata": {},
   "source": [
    "## Calcualte the mean and std of training dataset\n",
    "\n",
    "https://deeplizard.com/learn/video/lu7TCu7HeYc\n",
    "\n",
    "https://discuss.pytorch.org/t/understanding-transform-normalize/21730"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "74295564",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " ------------------------ \n",
      " before DataLoader @ Time = 13:30:21\n",
      "vocab is not None: assigning only @ Time = 13:30:21\n",
      "\n",
      "  _     ._   __/__   _ _  _  _ _/_   Recorded: 13:30:21  Samples:  439\n",
      " /_//_/// /_\\ / //_// / //_'/ //     Duration: 0.929     CPU time: 1.148\n",
      "/   _/                      v4.1.1\n",
      "\n",
      "Program: /home/salnabulsi/.thesis-py38/lib/python3.8/site-packages/ipykernel_launcher.py -f /home/salnabulsi/.local/share/jupyter/runtime/kernel-1e6baaff-dd1e-4fc8-b028-d4ee8f28de14.json\n",
      "\n",
      "0.928 run_code  IPython/core/interactiveshell.py:3403\n",
      "└─ 0.928 <module>  ../../../tmp/ipykernel_3390567/2968651614.py:5\n",
      "   └─ 0.928 get_loader  src/baseline/data_loader.py:11\n",
      "      └─ 0.928 __init__  src/baseline/coa_dataset.py:13\n",
      "         └─ 0.920 load_images  src/baseline/coa_dataset.py:39\n",
      "            ├─ 0.341 __call__  torchvision/transforms/transforms.py:90\n",
      "            │     [42 frames hidden]  torchvision, <built-in>, PIL\n",
      "            ├─ 0.332 convert  PIL/Image.py:873\n",
      "            │     [19 frames hidden]  PIL, <built-in>\n",
      "            │        0.314 ImagingDecoder.decode  <built-in>:0\n",
      "            └─ 0.244 open  PIL/Image.py:2925\n",
      "                  [6 frames hidden]  PIL, <built-in>\n",
      "\n",
      "\n",
      "\n",
      " ------------------------ \n",
      " after DataLoader @ Time = 13:30:22\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print_time('\\n ------------------------ \\n before DataLoader')\n",
    "profiler = Profiler()\n",
    "\n",
    "profiler.start()\n",
    "train_loader, train_dataset = get_loader(\n",
    "    root_folder=root_folder_images,\n",
    "    annotation_file=train_annotation_file,\n",
    "    transform=None,  # <=======================\n",
    "    num_workers=NUM_WORKER,\n",
    "    vocab=vocab,\n",
    "    batch_size=128,\n",
    "    device=device,\n",
    "    pin_memory=False\n",
    ")\n",
    "\n",
    "profiler.stop()\n",
    "\n",
    "profiler.print()\n",
    "\n",
    "print_time('\\n ------------------------ \\n after DataLoader')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "173f61ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_m(loader):\n",
    "    mean = 0.\n",
    "    std = 0.\n",
    "    for images, _ in loader:\n",
    "        batch_samples = images.size(0) # batch size (the last batch can have smaller size!)\n",
    "        images = images.view(batch_samples, images.size(1), -1)\n",
    "        mean += images.mean(2).sum(0)\n",
    "        std += images.std(2).sum(0)\n",
    "\n",
    "    mean /= len(loader.dataset)\n",
    "    std /= len(loader.dataset)\n",
    "    \n",
    "    return mean, std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6722cc45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " ------------------------ \n",
      " before get_mean_std @ Time = 13:30:24\n",
      "mean, std: tensor(1.0439) tensor(1.3915)\n",
      "\n",
      "  _     ._   __/__   _ _  _  _ _/_   Recorded: 13:30:21  Samples:  484\n",
      " /_//_/// /_\\ / //_// / //_'/ //     Duration: 2.977     CPU time: 2.068\n",
      "/   _/                      v4.1.1\n",
      "\n",
      "Program: /home/salnabulsi/.thesis-py38/lib/python3.8/site-packages/ipykernel_launcher.py -f /home/salnabulsi/.local/share/jupyter/runtime/kernel-1e6baaff-dd1e-4fc8-b028-d4ee8f28de14.json\n",
      "\n",
      "2.975 run_code  IPython/core/interactiveshell.py:3403\n",
      "├─ 2.046 <module>  ../../../tmp/ipykernel_3390567/2842918350.py:4\n",
      "│  ├─ 2.014 get_mean_std  src/baseline/data_loader.py:52\n",
      "│  │  ├─ 1.197 __next__  torch/utils/data/dataloader.py:517\n",
      "│  │  │     [45 frames hidden]  torch, multiprocessing, selectors, <b...\n",
      "│  │  │        1.014 poll.poll  <built-in>:0\n",
      "│  │  ├─ 0.501 __iter__  torch/utils/data/dataloader.py:346\n",
      "│  │  │     [68 frames hidden]  torch, multiprocessing, <built-in>, i...\n",
      "│  │  ├─ 0.155 [self]  \n",
      "│  │  ├─ 0.112 Tensor.pow  <built-in>:0\n",
      "│  │  │     [2 frames hidden]  <built-in>\n",
      "│  │  └─ 0.049 Tensor.sum  <built-in>:0\n",
      "│  │        [2 frames hidden]  <built-in>\n",
      "│  └─ 0.031 [self]  \n",
      "└─ 0.928 <module>  ../../../tmp/ipykernel_3390567/2968651614.py:5\n",
      "   └─ 0.928 get_loader  src/baseline/data_loader.py:11\n",
      "      └─ 0.928 __init__  src/baseline/coa_dataset.py:13\n",
      "         └─ 0.920 load_images  src/baseline/coa_dataset.py:39\n",
      "            ├─ 0.341 __call__  torchvision/transforms/transforms.py:90\n",
      "            │     [42 frames hidden]  torchvision, <built-in>, PIL\n",
      "            ├─ 0.332 convert  PIL/Image.py:873\n",
      "            │     [19 frames hidden]  PIL, <built-in>\n",
      "            └─ 0.244 open  PIL/Image.py:2925\n",
      "                  [6 frames hidden]  PIL, <built-in>\n",
      "\n",
      "\n",
      "\n",
      " ------------------------ \n",
      " after get_mean_std @ Time = 13:30:26\n"
     ]
    }
   ],
   "source": [
    "print_time('\\n ------------------------ \\n before get_mean_std')\n",
    "\n",
    "profiler.start()\n",
    "mean, std = get_mean_std(train_dataset, train_loader, 500 , 500)\n",
    "print('mean, std:', mean, std)\n",
    "profiler.stop()\n",
    "\n",
    "profiler.print()\n",
    "\n",
    "print_time('\\n ------------------------ \\n after get_mean_std')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "88ed54bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before @ Time = 13:30:27\n",
      "mean, std: tensor([0.3398, 0.3584, 0.3457]) tensor([0.3606, 0.3647, 0.3517])\n",
      "after @ Time = 13:30:28\n"
     ]
    }
   ],
   "source": [
    "print_time('before')\n",
    "mean, std = get_m(train_loader )\n",
    "print('mean, std:', mean, std)\n",
    "print_time('after')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "7a51a352",
   "metadata": {},
   "outputs": [],
   "source": [
    "#defining the transform to be applied\n",
    "\n",
    "transform = T.Compose([\n",
    "    T.Resize(226),                     \n",
    "    T.RandomCrop(224),                 \n",
    "    T.ToTensor(),                               \n",
    "    T.Normalize(mean, std) \n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d3e2222f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab is not None: assigning only @ Time = 13:30:29\n",
      "vocab is not None: assigning only @ Time = 13:30:30\n",
      "vocab is not None: assigning only @ Time = 13:30:30\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(2, 1, 1, 82, 27, 28)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Initiate the Dataset and Dataloader\n",
    "BATCH_SIZE = 56\n",
    "\n",
    "# print_time('writing the dataloader')\n",
    "\n",
    "train_loader, val_loader, test_loader, train_dataset, val_dataset, test_dataset = get_loaders(\n",
    "    root_folder=root_folder_images,\n",
    "    train_annotation_file=train_annotation_file,\n",
    "    val_annotation_file=val_annotation_file,\n",
    "    test_annotation_file=test_annotation_file,\n",
    "    transform=transform,\n",
    "    num_workers=NUM_WORKER,\n",
    "    vocab=vocab,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    device=device,\n",
    "    pin_memory=False\n",
    ")\n",
    "len(train_loader), len(val_loader), len(test_loader), len(train_dataset), len(val_dataset), len(test_dataset)\n",
    "# print_time('finished writing the dataloader')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "bc829f4c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'<PAD>': 0,\n",
       " '<SOS>': 1,\n",
       " '<EOS>': 2,\n",
       " '<UNK>': 3,\n",
       " 'g': 4,\n",
       " 'v': 5,\n",
       " 'b': 6,\n",
       " 'cross': 7,\n",
       " 'lion': 8,\n",
       " 'passt': 9,\n",
       " 's': 10,\n",
       " 'a': 11,\n",
       " 'eagle': 12,\n",
       " 'o': 13,\n",
       " 'doubleheaded': 14,\n",
       " \"'s\": 15,\n",
       " 'head': 16,\n",
       " 'patonce': 17,\n",
       " 'moline': 18,\n",
       " 'guard': 19,\n",
       " 'rampant': 20}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset.vocab.stoi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83bb6b3a",
   "metadata": {},
   "source": [
    "# Defining the Model Architecture\n",
    "\n",
    "Model is seq2seq model. In the **encoder** pretrained ResNet model is used to extract the features. Decoder, is the implementation of the Bahdanau Attention Decoder. In the decoder model **LSTM cell**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "332143d9",
   "metadata": {},
   "source": [
    "https://blog.floydhub.com/attention-mechanism/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d939ae8d",
   "metadata": {},
   "source": [
    "https://machinelearningmastery.com/develop-a-deep-learning-caption-generation-model-in-python/\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5808f315",
   "metadata": {},
   "source": [
    "### Setting Hypperparameter and Init the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "47a69030",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Hyperparams\n",
    "embed_size=300\n",
    "vocab_size = len(train_dataset.vocab)\n",
    "attention_dim=256\n",
    "encoder_dim=2048\n",
    "decoder_dim=512\n",
    "learning_rate = 3e-4\n",
    "drop_prob=0.3\n",
    "ignored_idx = train_dataset.vocab.stoi[\"<PAD>\"]\n",
    "\n",
    "hyper_params = {'embed_size': embed_size,\n",
    "                'attention_dim': attention_dim,\n",
    "                'encoder_dim': encoder_dim,\n",
    "                'decoder_dim': decoder_dim,\n",
    "                'vocap_size': vocab_size\n",
    "              }\n",
    "#initialize new model, loss etc\n",
    "model, optimizer, criterion = get_new_model(embed_size, vocab_size, attention_dim, encoder_dim,\n",
    "                                            decoder_dim, learning_rate,drop_prob,ignored_idx, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "781b80e8",
   "metadata": {},
   "source": [
    "## Training Job from above configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "53074260",
   "metadata": {},
   "outputs": [],
   "source": [
    "!export CUDA_LAUNCH_BLOCKING=1\n",
    "!export CUDA_VISIBLE_DEVICES=2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "fc2bb5a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|███████████████████████████████████████████| 2/2 [00:02<00:00,  1.14s/batch, loss=2.67]\n",
      "Epoch 2: 100%|███████████████████████████████████████████| 2/2 [00:02<00:00,  1.10s/batch, loss=2.29]\n",
      "Epoch 3: 100%|███████████████████████████████████████████| 2/2 [00:02<00:00,  1.10s/batch, loss=2.03]\n",
      "Epoch 4: 100%|███████████████████████████████████████████| 2/2 [00:02<00:00,  1.15s/batch, loss=1.74]\n",
      "Epoch 5: 100%|███████████████████████████████████████████| 2/2 [00:02<00:00,  1.11s/batch, loss=1.47]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bleu Score:  6.806270113745936e-05\n",
      "Final accuracy:  33.35\n"
     ]
    }
   ],
   "source": [
    "losses = list()\n",
    "losses_batch = list()\n",
    "val_losses = list()\n",
    "accuracy_list = list()\n",
    "\n",
    "model_full_path = '/home/space/datasets/COA/models/baseline/attention_model_dec22.pth'\n",
    "num_epochs = 5\n",
    "print_every = 5\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "# if model is None:\n",
    "\n",
    "for epoch in range(1, num_epochs + 1): \n",
    "#     if model is None:\n",
    "#         model, optimizer, epoch, loss = load_model_checkpoint(model_full_path)\n",
    "\n",
    "    with tqdm(train_loader, unit=\"batch\") as tepoch:\n",
    "        idx = 0\n",
    "        for image, captions in tepoch:\n",
    "            idx+=1\n",
    "            tepoch.set_description(f\"Epoch {epoch}\")\n",
    "            image, captions = image.to(device), captions.to(device)\n",
    "\n",
    "            # Zero the gradients.\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Feed forward\n",
    "            outputs, attentions = model(image, captions.T)\n",
    "            \n",
    "            # Calculate the batch loss.\n",
    "            targets = captions.T[:,1:]  ####### the fix in here\n",
    "#             print(targets)\n",
    "            loss = criterion(outputs.view(-1, vocab_size), targets.reshape(-1))\n",
    "#             print(outputs.view(-1, vocab_size))\n",
    "            \n",
    "            # Backward pass. \n",
    "            loss.backward()\n",
    "            \n",
    "            # Update the parameters in the optimizer.\n",
    "            optimizer.step()\n",
    "            \n",
    "            tepoch.set_postfix(loss=loss.item())\n",
    "            sleep(0.1)\n",
    " \n",
    "            avg_val_loss, bleu_score, accuracy = validate_model(model, criterion, val_loader, val_dataset, vocab_size, device)\n",
    "            model.train()\n",
    "#             tepoch.set_postfix(accuracy=accuracy)\n",
    "#             tepoch.set_postfix(vallidation_loss=avg_val_loss)\n",
    "\n",
    "            losses_batch.append(loss) # in here 17 batches * 5 epochs = 85 , you can get the average\n",
    "            val_losses.append(avg_val_loss)\n",
    "            accuracy_list.append(accuracy)\n",
    "        \n",
    "        avg_batch_loss = sum(losses_batch) / len(losses_batch)\n",
    "        losses.append(avg_batch_loss)\n",
    "            \n",
    "accuracy = sum(accuracy_list)/len(accuracy_list)\n",
    "\n",
    "print('Bleu Score: ',bleu_score/8091)\n",
    "print('Final accuracy: ', accuracy)\n",
    "\n",
    "                \n",
    "# save the latest model\n",
    "save_model(model, optimizer, epoch, loss, accuracy, model_full_path, hyper_params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "068ba422",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# losses = list()\n",
    "# for epoch in tqdm(range(5)):\n",
    "#     epoch_losses = train_epoch(model, device, train_loader, optimizer)\n",
    "#     print(f\"Average loss in epoch {epoch}: {np.mean(epoch_losses):.5f}\")\n",
    "#     losses.extend(epoch_losses)\n",
    "len(losses)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "eaeeaedc",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "can't convert cuda:0 device type tensor to numpy. Use Tensor.cpu() to copy the tensor to host memory first.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_3390567/2601399027.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlosses\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mxlabel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Iteration\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mylabel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Train loss\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.thesis-py38/lib/python3.8/site-packages/matplotlib/pyplot.py\u001b[0m in \u001b[0;36mplot\u001b[0;34m(scalex, scaley, data, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2755\u001b[0m \u001b[0;34m@\u001b[0m\u001b[0m_copy_docstring_and_deprecators\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mAxes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2756\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscalex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscaley\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2757\u001b[0;31m     return gca().plot(\n\u001b[0m\u001b[1;32m   2758\u001b[0m         \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscalex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mscalex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscaley\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mscaley\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2759\u001b[0m         **({\"data\": data} if data is not None else {}), **kwargs)\n",
      "\u001b[0;32m~/.thesis-py38/lib/python3.8/site-packages/matplotlib/axes/_axes.py\u001b[0m in \u001b[0;36mplot\u001b[0;34m(self, scalex, scaley, data, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1630\u001b[0m         \"\"\"\n\u001b[1;32m   1631\u001b[0m         \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcbook\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormalize_kwargs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmlines\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLine2D\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1632\u001b[0;31m         \u001b[0mlines\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_lines\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1633\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlines\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1634\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_line\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.thesis-py38/lib/python3.8/site-packages/matplotlib/axes/_base.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, data, *args, **kwargs)\u001b[0m\n\u001b[1;32m    310\u001b[0m                 \u001b[0mthis\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    311\u001b[0m                 \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 312\u001b[0;31m             \u001b[0;32myield\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_plot_args\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mthis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    313\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    314\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_next_color\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.thesis-py38/lib/python3.8/site-packages/matplotlib/axes/_base.py\u001b[0m in \u001b[0;36m_plot_args\u001b[0;34m(self, tup, kwargs, return_kwargs)\u001b[0m\n\u001b[1;32m    488\u001b[0m             \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_check_1d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxy\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    489\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 490\u001b[0;31m             \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mindex_of\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxy\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    491\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    492\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maxes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mxaxis\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.thesis-py38/lib/python3.8/site-packages/matplotlib/cbook/__init__.py\u001b[0m in \u001b[0;36mindex_of\u001b[0;34m(y)\u001b[0m\n\u001b[1;32m   1650\u001b[0m         \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1651\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1652\u001b[0;31m         \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_check_1d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1653\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mVisibleDeprecationWarning\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1654\u001b[0m         \u001b[0;31m# NumPy 1.19 will warn on ragged input, and we can't actually use it.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.thesis-py38/lib/python3.8/site-packages/matplotlib/cbook/__init__.py\u001b[0m in \u001b[0;36m_check_1d\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m   1302\u001b[0m     \u001b[0;34m\"\"\"Convert scalars to 1D arrays; pass-through arrays as is.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1303\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'shape'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1304\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0matleast_1d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1305\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1306\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36matleast_1d\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;32m~/.thesis-py38/lib/python3.8/site-packages/numpy/core/shape_base.py\u001b[0m in \u001b[0;36matleast_1d\u001b[0;34m(*arys)\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mary\u001b[0m \u001b[0;32min\u001b[0m \u001b[0marys\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m         \u001b[0mary\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0masanyarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mary\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mary\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mary\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.thesis-py38/lib/python3.8/site-packages/torch/_tensor.py\u001b[0m in \u001b[0;36m__array__\u001b[0;34m(self, dtype)\u001b[0m\n\u001b[1;32m    676\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mhandle_torch_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__array__\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    677\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 678\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    679\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    680\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: can't convert cuda:0 device type tensor to numpy. Use Tensor.cpu() to copy the tensor to host memory first."
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(losses)\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.ylabel(\"Train loss\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "78c901cf",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "can't convert cuda:0 device type tensor to numpy. Use Tensor.cpu() to copy the tensor to host memory first.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_3390567/2774624419.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_losses\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mxlabel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Iteration\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mylabel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Validation loss\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.thesis-py38/lib/python3.8/site-packages/matplotlib/pyplot.py\u001b[0m in \u001b[0;36mplot\u001b[0;34m(scalex, scaley, data, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2755\u001b[0m \u001b[0;34m@\u001b[0m\u001b[0m_copy_docstring_and_deprecators\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mAxes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2756\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscalex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscaley\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2757\u001b[0;31m     return gca().plot(\n\u001b[0m\u001b[1;32m   2758\u001b[0m         \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscalex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mscalex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscaley\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mscaley\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2759\u001b[0m         **({\"data\": data} if data is not None else {}), **kwargs)\n",
      "\u001b[0;32m~/.thesis-py38/lib/python3.8/site-packages/matplotlib/axes/_axes.py\u001b[0m in \u001b[0;36mplot\u001b[0;34m(self, scalex, scaley, data, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1630\u001b[0m         \"\"\"\n\u001b[1;32m   1631\u001b[0m         \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcbook\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormalize_kwargs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmlines\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLine2D\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1632\u001b[0;31m         \u001b[0mlines\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_lines\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1633\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlines\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1634\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_line\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.thesis-py38/lib/python3.8/site-packages/matplotlib/axes/_base.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, data, *args, **kwargs)\u001b[0m\n\u001b[1;32m    310\u001b[0m                 \u001b[0mthis\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    311\u001b[0m                 \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 312\u001b[0;31m             \u001b[0;32myield\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_plot_args\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mthis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    313\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    314\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_next_color\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.thesis-py38/lib/python3.8/site-packages/matplotlib/axes/_base.py\u001b[0m in \u001b[0;36m_plot_args\u001b[0;34m(self, tup, kwargs, return_kwargs)\u001b[0m\n\u001b[1;32m    488\u001b[0m             \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_check_1d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxy\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    489\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 490\u001b[0;31m             \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mindex_of\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxy\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    491\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    492\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maxes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mxaxis\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.thesis-py38/lib/python3.8/site-packages/matplotlib/cbook/__init__.py\u001b[0m in \u001b[0;36mindex_of\u001b[0;34m(y)\u001b[0m\n\u001b[1;32m   1650\u001b[0m         \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1651\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1652\u001b[0;31m         \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_check_1d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1653\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mVisibleDeprecationWarning\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1654\u001b[0m         \u001b[0;31m# NumPy 1.19 will warn on ragged input, and we can't actually use it.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.thesis-py38/lib/python3.8/site-packages/matplotlib/cbook/__init__.py\u001b[0m in \u001b[0;36m_check_1d\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m   1302\u001b[0m     \u001b[0;34m\"\"\"Convert scalars to 1D arrays; pass-through arrays as is.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1303\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'shape'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1304\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0matleast_1d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1305\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1306\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36matleast_1d\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;32m~/.thesis-py38/lib/python3.8/site-packages/numpy/core/shape_base.py\u001b[0m in \u001b[0;36matleast_1d\u001b[0;34m(*arys)\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mary\u001b[0m \u001b[0;32min\u001b[0m \u001b[0marys\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m         \u001b[0mary\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0masanyarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mary\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mary\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mary\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.thesis-py38/lib/python3.8/site-packages/torch/_tensor.py\u001b[0m in \u001b[0;36m__array__\u001b[0;34m(self, dtype)\u001b[0m\n\u001b[1;32m    676\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mhandle_torch_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__array__\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    677\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 678\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    679\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    680\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: can't convert cuda:0 device type tensor to numpy. Use Tensor.cpu() to copy the tensor to host memory first."
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(val_losses)\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.ylabel(\"Validation loss\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6821e855",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(accuracy_list)\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.ylabel(\"Validation Accuracy\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b22f6fee",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1927f99b",
   "metadata": {},
   "source": [
    "##  Visualizing the attentions\n",
    "Defining helper functions\n",
    "<li>Given the image generate captions and attention scores</li>\n",
    "<li>Plot the attention scores in the image</li>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "1cac2ecf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#generate caption\n",
    "def get_caps_from(model, features_tensors):\n",
    "    #generate the caption\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        features = model.encoder(features_tensors.to(device))\n",
    "        caps,alphas = model.decoder.generate_caption(features,vocab=test_dataset.vocab)\n",
    "        caption = ' '.join(caps)\n",
    "        show_image(features_tensors[0],title=caption)\n",
    "    \n",
    "    return caps,alphas\n",
    "\n",
    "#Show attention\n",
    "def plot_attention(img, result, attention_plot):\n",
    "    #untransform\n",
    "    img[0] = img[0] * 0.229\n",
    "    img[1] = img[1] * 0.224 \n",
    "    img[2] = img[2] * 0.225 \n",
    "    img[0] += 0.485 \n",
    "    img[1] += 0.456 \n",
    "    img[2] += 0.406\n",
    "    \n",
    "    img = img.numpy().transpose((1, 2, 0))\n",
    "    temp_image = img\n",
    "\n",
    "    fig = plt.figure(figsize=(15, 15))\n",
    "\n",
    "    len_result = len(result)\n",
    "    for l in range(len_result):\n",
    "        temp_att = attention_plot[l].reshape(7,7)\n",
    "        \n",
    "        ax = fig.add_subplot(len_result//2,len_result//2, l+1)\n",
    "        ax.set_title(result[l])\n",
    "        img = ax.imshow(temp_image)\n",
    "        ax.imshow(temp_att, cmap='gray', alpha=0.7, extent=img.get_extent())\n",
    "        \n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "def test_rand_image(model):\n",
    "    dataiter = iter(test_loader)\n",
    "    images,_ = next(dataiter)\n",
    "\n",
    "    img = images[0].detach().clone()\n",
    "    img1 = images[0].detach().clone()\n",
    "    caps,alphas = get_caps_from(model, img.unsqueeze(0))\n",
    "\n",
    "    plot_attention(img1, caps, alphas)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebe98616",
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.cuda.empty_cache()\n",
    "\n",
    "# model_path = '/home/space/datasets/COA/models/baseline/attention_model_state.pth'\n",
    "# local_model_path = 'simple-model.pth'\n",
    "# model = load_model(local_model_path)\n",
    "test_rand_image(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4619980c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
